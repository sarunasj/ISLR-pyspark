{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> Define SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"unsupervised learning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "\n",
    "import utils\n",
    "from pyspark.ml.feature import StandardScaler, PCA\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Lab 1: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "USArrests data:\n",
      "+------+-------+--------+----+\n",
      "|Murder|Assault|UrbanPop|Rape|\n",
      "+------+-------+--------+----+\n",
      "|  13.2|    236|      58|21.2|\n",
      "|  10.0|    263|      48|44.5|\n",
      "|   8.1|    294|      80|31.0|\n",
      "|   8.8|    190|      50|19.5|\n",
      "|   9.0|    276|      91|40.6|\n",
      "+------+-------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Summary table:\n",
      "+-------+-----------------+----------------+------------------+------------------+\n",
      "|summary|           Murder|         Assault|          UrbanPop|              Rape|\n",
      "+-------+-----------------+----------------+------------------+------------------+\n",
      "|  count|               50|              50|                50|                50|\n",
      "|   mean|7.787999999999999|          170.76|             65.54|21.231999999999992|\n",
      "| stddev|4.355509764209288|83.3376608400171|14.474763400836784|  9.36638453105965|\n",
      "|    min|              0.8|              45|                32|               7.3|\n",
      "|    max|             17.4|             337|                91|              46.0|\n",
      "+-------+-----------------+----------------+------------------+------------------+\n",
      "\n",
      "\n",
      "Data types:\n",
      "root\n",
      " |-- Murder: double (nullable = true)\n",
      " |-- Assault: integer (nullable = true)\n",
      " |-- UrbanPop: integer (nullable = true)\n",
      " |-- Rape: double (nullable = true)\n",
      "\n",
      "\n",
      "PCA feature values:\n",
      "+----------------------------------------+\n",
      "|pcaFeatures                             |\n",
      "+----------------------------------------+\n",
      "|[-0.9756604483336044,1.1220012104334096]|\n",
      "|[-1.930537878513682,1.0624269195344451] |\n",
      "|[-1.745442853390598,-0.7384595372849996]|\n",
      "|[0.13999894434859508,1.1085422595059098]|\n",
      "|[-2.49861284825858,-1.5274267208247936] |\n",
      "+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Explained variance by principal components:\n",
      "[0.6200603947873732,0.24744128813496033]\n",
      "\n",
      "Principal components loadings:\n",
      "DenseMatrix([[-0.53589947,  0.41818087],\n",
      "             [-0.27819087, -0.87280619],\n",
      "             [-0.54343209, -0.16731864],\n",
      "             [-0.58318363,  0.1879856 ]])\n"
     ]
    }
   ],
   "source": [
    "# -> USArrests data:\n",
    "\n",
    "USArrests = spark.read.csv('data/USArrests.csv',inferSchema=True,header=True).drop('_c0')\n",
    "states = USArrests.columns\n",
    "\n",
    "print('\\nUSArrests data:'); USArrests.show(5)\n",
    "print('\\nSummary table:'); USArrests.describe().show()\n",
    "print('\\nData types:'); USArrests.printSchema()\n",
    "\n",
    "# -> Prepare data:\n",
    "\n",
    "data = utils.prepare_data(df = USArrests,\n",
    "                    labelCol = None,\n",
    "                    label_is_categorical = None,\n",
    "                    categoricalCols = [],\n",
    "                    continuousCols = ['Murder', 'UrbanPop', 'Rape', 'Assault']\n",
    "                   )\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance:\n",
    "scaler = StandardScaler(inputCol=\"features\", \n",
    "                        outputCol=\"scaledFeatures\",\n",
    "                        withStd=True, \n",
    "                        withMean=True)\n",
    "\n",
    "data = scaler.fit(data).transform(data)\n",
    "\n",
    "# -> Describe and fit the model\n",
    "\n",
    "model = PCA(k=2, inputCol=\"scaledFeatures\", outputCol=\"pcaFeatures\")\n",
    "model_fit = model.fit(data)\n",
    "\n",
    "# -> Print results:\n",
    "\n",
    "result = model_fit.transform(data).select(\"pcaFeatures\")\n",
    "print('\\nPCA feature values:'); result.show(5, truncate=False)\n",
    "\n",
    "print('\\nExplained variance by principal components:'); print(model_fit.explainedVariance)\n",
    "print('\\nPrincipal components loadings:'); print(model_fit.pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5. Lab 2: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *10.5.1 K-Means Clustering*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "+------------------+------------------+\n",
      "|                x1|                x2|\n",
      "+------------------+------------------+\n",
      "| 2.746122098640372|-4.220500456852838|\n",
      "| 3.000839055656425|-4.670859651663419|\n",
      "|2.9134504593955683|-4.085528175217294|\n",
      "| 4.407107624532321|-5.397542877723977|\n",
      "|3.6810251804138696|-3.059545273554204|\n",
      "+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Within Set Sum of Squared Errors = 60.794\n",
      "\n",
      "Cluster Centers: \n",
      "[0.07989059 0.62020926]\n",
      "[ 3.09812141 -3.8719231 ]\n",
      "[ 0.07953395 -1.1674171 ]\n",
      "\n",
      " Predictions:\n",
      "+------------------+------------------+--------------------+----------+\n",
      "|                x1|                x2|            features|prediction|\n",
      "+------------------+------------------+--------------------+----------+\n",
      "| 2.746122098640372|-4.220500456852838|[2.74612209864037...|         1|\n",
      "| 3.000839055656425|-4.670859651663419|[3.00083905565642...|         1|\n",
      "|2.9134504593955683|-4.085528175217294|[2.91345045939556...|         1|\n",
      "| 4.407107624532321|-5.397542877723977|[4.40710762453232...|         1|\n",
      "|3.6810251804138696|-3.059545273554204|[3.68102518041386...|         1|\n",
      "+------------------+------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -> Generate data:\n",
    "\n",
    "n = 50\n",
    "x = np.random.normal(0, 1, (n,2))\n",
    "\n",
    "x[:25, 0] = x[:25,0] + 3\n",
    "x[:25, 1] = x[:25,1] - 4\n",
    "\n",
    "pd_data = pd.DataFrame(x,columns=['x1','x2'])\n",
    "data = spark.createDataFrame(pd_data)\n",
    "print('Data:'); data.show(5)\n",
    "\n",
    "# -> Prepare data:\n",
    "\n",
    "data = utils.prepare_data(df = data,\n",
    "                    labelCol = None,\n",
    "                    label_is_categorical = None,\n",
    "                    categoricalCols = [],\n",
    "                    continuousCols = ['x1', 'x2']\n",
    "                   )\n",
    "\n",
    "# Train K-means model\n",
    "\n",
    "K = 3\n",
    "model = KMeans(k=5).setK(K).setSeed(1)\n",
    "model_fit = model.fit(data)\n",
    "\n",
    "# -> Evaluate clustering by computing Within Set Sum of Squared Errors:\n",
    "wssse = model_fit.computeCost(data)\n",
    "print(\"Within Set Sum of Squared Errors = {:.3f}\".format(wssse))\n",
    "\n",
    "# -> print the result:\n",
    "centers = model_fit.clusterCenters()\n",
    "print(\"\\nCluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "    \n",
    "# -> Predictions\n",
    "print('\\n Predictions:'); model_fit.transform(data).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *10.5.2 Hierachical Clustering*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Not implemented in MLlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
